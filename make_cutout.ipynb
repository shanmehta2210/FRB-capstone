{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Using the given python script to generate a cutout of NGC3379 and convert the inverse variance map to sigma map"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from astropy.io import fits\n",
                "from astropy.wcs import WCS\n",
                "from astropy.nddata import Cutout2D\n",
                "from astropy.coordinates import SkyCoord\n",
                "import astropy.units as u\n",
                "import numpy as np\n",
                "\n",
                "# -------------------------------------------------\n",
                "# Input brick files (you must edit these paths)\n",
                "# -------------------------------------------------\n",
                "image_file  = \"/mnt/c/Users/lenovo/Desktop/FRB-capstone/legacysurvey-1619p125-image-r.fits.fz\"\n",
                "invvar_file = \"/mnt/c/Users/lenovo/Desktop/FRB-capstone/legacysurvey-1619p125-invvar-r.fits.fz\"\n",
                "\n",
                "\n",
                "# -------------------------------------------------\n",
                "# Target coordinates: NGC 3379 (M105)\n",
                "# -------------------------------------------------\n",
                "center = SkyCoord(ra=161.9567 * u.deg, dec=12.5817 * u.deg, frame=\"icrs\")\n",
                "\n",
                "# Cutout size on the sky\n",
                "size = (200 * u.arcsec, 200 * u.arcsec)  # adjust if needed\n",
                "\n",
                "# Output filenames\n",
                "out_sci   = \"NGC3379_r_cutout.fits\"\n",
                "out_sigma = \"NGC3379_r_sigma_cutout.fits\"\n",
                "\n",
                "# -------------------------------------------------\n",
                "# Read the brick images (note: data stored in HDU 1)\n",
                "# -------------------------------------------------\n",
                "with fits.open(image_file) as hdul_img, fits.open(invvar_file) as hdul_inv:\n",
                "    img_hdu = hdul_img[1]      # science image\n",
                "    inv_hdu = hdul_inv[1]      # inverse variance image\n",
                "\n",
                "    data_img = img_hdu.data\n",
                "    data_inv = inv_hdu.data\n",
                "\n",
                "    # WCS from the science header\n",
                "    wcs_full = WCS(img_hdu.header)\n",
                "\n",
                "    # -------------------------------------------------\n",
                "    # 1. Make science and invvar cutouts\n",
                "    # -------------------------------------------------\n",
                "    cutout_img = Cutout2D(data_img, position=center, size=size, wcs=wcs_full)\n",
                "    cutout_inv = Cutout2D(data_inv, position=center, size=size, wcs=wcs_full)\n",
                "\n",
                "    # -------------------------------------------------\n",
                "    # 2. Convert invvar → sigma map\n",
                "    #    invvar = 1/sigma^2  => sigma = 1/sqrt(invvar)\n",
                "    # -------------------------------------------------\n",
                "    inv = cutout_inv.data\n",
                "    sigma = np.full(inv.shape, 1e9, dtype=float) # safer default for zero invvar\n",
                "    mask = inv > 0\n",
                "    sigma[mask] = 1.0 / np.sqrt(inv[mask])\n",
                "\n",
                "    # -------------------------------------------------\n",
                "    # 3. Write output FITS files with WCS\n",
                "    # -------------------------------------------------\n",
                "    hdr_cut = cutout_img.wcs.to_header()\n",
                "\n",
                "    fits.PrimaryHDU(data=cutout_img.data, header=hdr_cut).writeto(out_sci, overwrite=True)\n",
                "    fits.PrimaryHDU(data=sigma, header=hdr_cut).writeto(out_sigma, overwrite=True)\n",
                "\n",
                "print(\"Wrote:\", out_sci)\n",
                "print(\"Wrote:\", out_sigma)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Using Sextractor to generate a binary fits file, with the information of isolated point stars and small image cutouts of these stars ( 35 by 35 ) pixels for now, which will be fed to psfex."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# edited default.sex to correct arcsec pixel ratio, to increase signal noise ratio(detect thresh) and to change file type of output\n",
                "# edited default.param to only contain values of interest and include 35by35 pixel cutouts for psfex\n",
                "# had to bring default.conv and default.nnw to current directory\n",
                "# ran sex legacysurvey-1619p125-image-r.fits.fz -c default.sex -CATALOG_NAME ForPSF.cat"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We verify that the CLASS_STAR is upheld through this python script. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- VERIFICATION REPORT: ForPSF.cat ---\n",
                        "Error: forcpsf.cat not found!\n"
                    ]
                },
                {
                    "ename": "NameError",
                    "evalue": "name 't' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[1], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: forcpsf.cat not found!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m     exit()\n\u001b[0;32m---> 22\u001b[0m total_objects \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mt\u001b[49m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Objects Detected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_objects\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# 1. Calculate Ellipticity from Elongation\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# SExtractor ELONGATION = A/B.   Ellipticity = 1 - (1/Elongation)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# (Note: PSFEx definition varies slightly, but this is the standard proxy)\u001b[39;00m\n",
                        "\u001b[0;31mNameError\u001b[0m: name 't' is not defined"
                    ]
                },
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
                        "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
                        "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
                        "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
                    ]
                }
            ],
            "source": [
                "from astropy.io import fits\n",
                "from astropy.table import Table\n",
                "import numpy as np\n",
                "\n",
                "# Define your strict cutoffs\n",
                "MIN_SNR = 30.0\n",
                "MAX_ELLIP = 0.2\n",
                "MIN_CLASS_STAR = 0.8  # The guideline from your PDF\n",
                "\n",
                "catalog_file = 'ForPSF.cat'\n",
                "clean_file = 'clean_stars.cat'\n",
                "\n",
                "print(f\"--- VERIFICATION REPORT: {catalog_file} ---\")\n",
                "\n",
                "# Load the catalog (Data is usually in HDU 2 for LDAC)\n",
                "try:\n",
                "    t = Table.read(catalog_file, hdu=2)\n",
                "except FileNotFoundError:\n",
                "    print(\"Error: forcpsf.cat not found!\")\n",
                "    exit()\n",
                "\n",
                "total_objects = len(t)\n",
                "print(f\"Total Objects Detected: {total_objects}\")\n",
                "\n",
                "# 1. Calculate Ellipticity from Elongation\n",
                "# SExtractor ELONGATION = A/B.   Ellipticity = 1 - (1/Elongation)\n",
                "# (Note: PSFEx definition varies slightly, but this is the standard proxy)\n",
                "ellipticity = (t['ELONGATION'] - 1.0)/(1.0 + t['ELONGATION'])\n",
                "\n",
                "# 2. Apply the 'Proxy' Filters (What PSFEx is doing)\n",
                "psfex_mask = (t['SNR_WIN'] > MIN_SNR) & (ellipticity < MAX_ELLIP) & (t['FLAGS'] == 0)\n",
                "psfex_survivors = t[psfex_mask]\n",
                "\n",
                "print(f\"\\n[Step 1] Applying PSFEx Filters (Roundness + SNR + Isolation):\")\n",
                "print(f\"   Survivors: {len(psfex_survivors)} / {total_objects}\")\n",
                "\n",
                "# 3. VERIFY: Do these survivors actually meet the CLASS_STAR > 0.8 requirement?\n",
                "bad_survivors = psfex_survivors[psfex_survivors['CLASS_STAR'] < MIN_CLASS_STAR]\n",
                "num_bad = len(bad_survivors)\n",
                "\n",
                "print(f\"\\n[Step 2] Verification Check:\")\n",
                "print(f\"   Of the {len(psfex_survivors)} stars PSFEx would pick...\")\n",
                "print(f\"   {num_bad} of them have CLASS_STAR < {MIN_CLASS_STAR} (FAIL)\")\n",
                "print(f\"   {len(psfex_survivors) - num_bad} of them have CLASS_STAR >= {MIN_CLASS_STAR} (PASS)\")\n",
                "\n",
                "if num_bad == 0:\n",
                "    print(\"\\n✅ SUCCESS: Your PSFEx settings perfectly match the CLASS_STAR guideline!\")\n",
                "else:\n",
                "    print(f\"\\n⚠️  WARNING: Your settings let in {num_bad} impostors (Round objects that aren't stars).\")\n",
                "\n",
                "# 4. ENFORCE: Create a \"Perfect\" Catalog\n",
                "# We filter by EVERYTHING: SNR + Ellipticity + FLAGS + CLASS_STAR\n",
                "final_mask = psfex_mask & (t['CLASS_STAR'] >= MIN_CLASS_STAR)\n",
                "clean_table = t[final_mask]\n",
                "\n",
                "# Save as a new FITS_LDAC file\n",
                "# We need to preserve the HDU structure for PSFEx to read it\n",
                "\"\"\" hdul = fits.open(catalog_file)\n",
                "hdul[2].data = clean_table.as_array() # Replace data with filtered version\n",
                "hdul.writeto(clean_file, overwrite=True) \"\"\"\n",
                "\n",
                "print(\"-\" * 50)\n",
                "print(f\"✅ CLEANED CATALOG SAVED: {clean_file}\")\n",
                "print(f\"   Contains {len(clean_table)} guaranteed stars.\")\n",
                "print(f\"   Action: Run 'psfex {clean_file}' to use this guaranteed sample.\")\n",
                "print(\"-\" * 50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now use PSFEx to generate a model of the telescope blur ( Point Spread Function ), we hope to find. We attempt FLAG = 0."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Changed snr > 30 over 20, Max_ellp = .2 over .3, and set FLAG to only include perfect stars\n",
                "# ran psfex ForPSF.cat\n",
                "# Analyzed the various statistic fits files"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now attempt to generate parameters through galfit"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "frb_project",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
