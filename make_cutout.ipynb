{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Using the given python script to generate a cutout of NGC3379 and convert the inverse variance map to sigma map"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Wrote: NGC3379_r_cutout.fits\n",
                        "Wrote: NGC3379_r_sigma_cutout.fits\n"
                    ]
                }
            ],
            "source": [
                "from astropy.io import fits\n",
                "from astropy.wcs import WCS\n",
                "from astropy.nddata import Cutout2D\n",
                "from astropy.coordinates import SkyCoord\n",
                "import astropy.units as u\n",
                "import numpy as np\n",
                "\n",
                "# -------------------------------------------------\n",
                "# Input brick files (you must edit these paths)\n",
                "# -------------------------------------------------\n",
                "image_file  = \"/mnt/c/Users/lenovo/Desktop/FRB-capstone/legacysurvey-1619p125-image-r.fits.fz\"\n",
                "invvar_file = \"/mnt/c/Users/lenovo/Desktop/FRB-capstone/legacysurvey-1619p125-invvar-r.fits.fz\"\n",
                "\n",
                "\n",
                "# -------------------------------------------------\n",
                "# Target coordinates: NGC 3379 (M105)\n",
                "# -------------------------------------------------\n",
                "center = SkyCoord(ra=161.9567 * u.deg, dec=12.5817 * u.deg, frame=\"icrs\")\n",
                "\n",
                "# Cutout size on the sky\n",
                "size = (200 * u.arcsec, 200 * u.arcsec)  # adjust if needed\n",
                "\n",
                "# Output filenames\n",
                "out_sci   = \"NGC3379_r_cutout.fits\"\n",
                "out_sigma = \"NGC3379_r_sigma_cutout.fits\"\n",
                "\n",
                "# -------------------------------------------------\n",
                "# Read the brick images (note: data stored in HDU 1)\n",
                "# -------------------------------------------------\n",
                "with fits.open(image_file) as hdul_img, fits.open(invvar_file) as hdul_inv:\n",
                "    img_hdu = hdul_img[1]      # science image\n",
                "    inv_hdu = hdul_inv[1]      # inverse variance image\n",
                "\n",
                "    data_img = img_hdu.data\n",
                "    data_inv = inv_hdu.data\n",
                "\n",
                "    # WCS from the science header\n",
                "    wcs_full = WCS(img_hdu.header)\n",
                "\n",
                "    # -------------------------------------------------\n",
                "    # 1. Make science and invvar cutouts\n",
                "    # -------------------------------------------------\n",
                "    cutout_img = Cutout2D(data_img, position=center, size=size, wcs=wcs_full)\n",
                "    cutout_inv = Cutout2D(data_inv, position=center, size=size, wcs=wcs_full)\n",
                "\n",
                "    # -------------------------------------------------\n",
                "    # 2. Convert invvar → sigma map\n",
                "    #    invvar = 1/sigma^2  => sigma = 1/sqrt(invvar)\n",
                "    # -------------------------------------------------\n",
                "    inv = cutout_inv.data\n",
                "    sigma = np.full(inv.shape, 1e9, dtype=float) # safer default for zero invvar\n",
                "    mask = inv > 0\n",
                "    sigma[mask] = 1.0 / np.sqrt(inv[mask])\n",
                "\n",
                "    # -------------------------------------------------\n",
                "    # 3. Write output FITS files with WCS\n",
                "    # -------------------------------------------------\n",
                "    hdr_cut = cutout_img.wcs.to_header()\n",
                "\n",
                "    # fits.PrimaryHDU(data=cutout_img.data, header=hdr_cut).writeto(out_sci, overwrite=True)\n",
                "    fits.PrimaryHDU(data=sigma, header=hdr_cut).writeto(out_sigma, overwrite=True)\n",
                "\n",
                "print(\"Wrote:\", out_sci)\n",
                "print(\"Wrote:\", out_sigma)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Reading NGC3379_r_nan_sigma_cutout.fits...\n",
                        "Found 423 pixels with NaN values.\n",
                        "Writing corrected data to NGC3379_r_sigma_FIXED.fits...\n",
                        "Done! Use this new file in your GALFIT feedme (Line C).\n"
                    ]
                }
            ],
            "source": [
                "import numpy as np\n",
                "from astropy.io import fits\n",
                "\n",
                "# --- CONFIGURATION ---\n",
                "input_filename = 'NGC3379_r_nan_sigma_cutout.fits'  # Your input file with NaNs\n",
                "output_filename = 'NGC3379_r_sigma_FIXED.fits'     # The new file to create\n",
                "fill_value = 1e9                                     # The value to replace NaNs with\n",
                "\n",
                "# --- PROCESSING ---\n",
                "print(f\"Reading {input_filename}...\")\n",
                "\n",
                "try:\n",
                "    # Open the FITS file\n",
                "    with fits.open(input_filename) as hdul:\n",
                "        # Load the data. usually in the primary HDU (index 0) for simple cutouts\n",
                "        # If your cutout script put it in extension 1, change this to hdul[1].data\n",
                "        data = hdul[0].data.astype(np.float32)\n",
                "        header = hdul[0].header\n",
                "\n",
                "        # Check for NaNs before fixing\n",
                "        nan_count = np.count_nonzero(np.isnan(data))\n",
                "        print(f\"Found {nan_count} pixels with NaN values.\")\n",
                "\n",
                "        # REPLACE NaNs with the fill value\n",
                "        # We use np.nan_to_num for safety, or direct boolean indexing\n",
                "        data[np.isnan(data)] = fill_value\n",
                "        \n",
                "        # OPTIONAL: Also replace <= 0 values if any exist (physically impossible for sigma)\n",
                "        zero_count = np.count_nonzero(data <= 0)\n",
                "        if zero_count > 0:\n",
                "            print(f\"Found {zero_count} pixels with <= 0 values. Fixing those too.\")\n",
                "            data[data <= 0] = fill_value\n",
                "\n",
                "        # Save the new file\n",
                "        print(f\"Writing corrected data to {output_filename}...\")\n",
                "        fits.PrimaryHDU(data=data, header=header).writeto(output_filename, overwrite=True)\n",
                "\n",
                "    print(\"Done! Use this new file in your GALFIT feedme (Line C).\")\n",
                "\n",
                "except FileNotFoundError:\n",
                "    print(f\"Error: Could not find file '{input_filename}'. Check the name.\")\n",
                "except Exception as e:\n",
                "    print(f\"An error occurred: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Reading NGC3379_r_nan_sigma_cutout.fits...\n",
                        "Flagged 423 NaN pixels.\n",
                        "Flagged 0 Infinite pixels.\n",
                        "Flagged 0 Zero/Negative pixels.\n",
                        "Saving mask to bad_pixel_mask.fits...\n",
                        "Done! Update 'Line F' in your galfit.feedme file.\n"
                    ]
                }
            ],
            "source": [
                "import numpy as np\n",
                "from astropy.io import fits\n",
                "\n",
                "# --- CONFIGURATION ---\n",
                "# Use the file that definitely has NaNs or Infinities in the center\n",
                "input_sigma_file = 'NGC3379_r_nan_sigma_cutout.fits' \n",
                "output_mask_file = 'bad_pixel_mask.fits'\n",
                "\n",
                "print(f\"Reading {input_sigma_file}...\")\n",
                "\n",
                "try:\n",
                "    with fits.open(input_sigma_file) as hdul:\n",
                "        # Load data (usually extension 0 or 1, check your file structure)\n",
                "        data = hdul[0].data\n",
                "        header = hdul[0].header\n",
                "\n",
                "        # Initialize mask with 0 (Good pixels)\n",
                "        # GALFIT convention: 0 = Good, >0 = Bad\n",
                "        mask = np.zeros_like(data, dtype=np.int16)\n",
                "\n",
                "        # Flag NaNs\n",
                "        nan_count = np.count_nonzero(np.isnan(data))\n",
                "        mask[np.isnan(data)] = 1\n",
                "        print(f\"Flagged {nan_count} NaN pixels.\")\n",
                "\n",
                "        # Flag Infinities (just in case)\n",
                "        inf_count = np.count_nonzero(np.isinf(data))\n",
                "        mask[np.isinf(data)] = 1\n",
                "        print(f\"Flagged {inf_count} Infinite pixels.\")\n",
                "        \n",
                "        # Optional: Flag Zero/Negative Sigma (Physical impossibility = Bad data)\n",
                "        zero_count = np.count_nonzero(data <= 0)\n",
                "        # Don't double count NaNs (comparisons with NaN are tricky)\n",
                "        # Use a safe mask for this check\n",
                "        valid_data_mask = ~np.isnan(data) & ~np.isinf(data)\n",
                "        zeros_to_flag = (data <= 0) & valid_data_mask\n",
                "        mask[zeros_to_flag] = 1\n",
                "        print(f\"Flagged {np.count_nonzero(zeros_to_flag)} Zero/Negative pixels.\")\n",
                "\n",
                "        # Save the mask\n",
                "        print(f\"Saving mask to {output_mask_file}...\")\n",
                "        fits.PrimaryHDU(data=mask, header=header).writeto(output_mask_file, overwrite=True)\n",
                "\n",
                "    print(\"Done! Update 'Line F' in your galfit.feedme file.\")\n",
                "\n",
                "except FileNotFoundError:\n",
                "    print(f\"Error: Could not find {input_sigma_file}\")\n",
                "except Exception as e:\n",
                "    print(f\"An error occurred: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Using Sextractor to generate a binary fits file, with the information of isolated point stars and small image cutouts of these stars ( 35 by 35 ) pixels for now, which will be fed to psfex."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# edited default.sex to correct arcsec pixel ratio, to increase signal noise ratio(detect thresh) and to change file type of output\n",
                "# edited default.param to only contain values of interest and include 35by35 pixel cutouts for psfex\n",
                "# had to bring default.conv and default.nnw to current directory\n",
                "# ran sex legacysurvey-1619p125-image-r.fits.fz -c default.sex -CATALOG_NAME ForPSF.cat"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We verify that the CLASS_STAR is upheld through this python script. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- CLASS_STAR CLEANER: ForPSF.cat ---\n",
                        "Error: ForPSF.cat not found!\n"
                    ]
                },
                {
                    "ename": "NameError",
                    "evalue": "name 't' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[3], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcatalog_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m     exit()\n\u001b[0;32m---> 20\u001b[0m total_objects \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mt\u001b[49m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Objects Detected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_objects\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# FILTER ONLY BY CLASS_STAR\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# We are ignoring SNR, Ellipticity, and Flags for now.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------\u001b[39;00m\n",
                        "\u001b[0;31mNameError\u001b[0m: name 't' is not defined"
                    ]
                },
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
                        "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
                        "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
                        "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
                    ]
                }
            ],
            "source": [
                "from astropy.io import fits\n",
                "from astropy.table import Table\n",
                "import numpy as np\n",
                "\n",
                "# Define your strict cutoff\n",
                "MIN_CLASS_STAR = 0.8  # The guideline from your PDF\n",
                "\n",
                "catalog_file = 'ForPSF.cat'\n",
                "clean_file = 'clean_stars.cat'\n",
                "\n",
                "print(f\"--- CLASS_STAR CLEANER: {catalog_file} ---\")\n",
                "\n",
                "# Load the catalog (Data is usually in HDU 2 for LDAC)\n",
                "try:\n",
                "    t = Table.read(catalog_file, hdu=2)\n",
                "except FileNotFoundError:\n",
                "    print(f\"Error: {catalog_file} not found!\")\n",
                "    exit()\n",
                "\n",
                "total_objects = len(t)\n",
                "print(f\"Total Objects Detected: {total_objects}\")\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# FILTER ONLY BY CLASS_STAR\n",
                "# We are ignoring SNR, Ellipticity, and Flags for now.\n",
                "# ---------------------------------------------------------\n",
                "final_mask = t['CLASS_STAR'] >= MIN_CLASS_STAR\n",
                "clean_table = t[final_mask]\n",
                "\n",
                "cleaned_count = len(clean_table)\n",
                "removed_count = total_objects - cleaned_count\n",
                "\n",
                "print(f\"Objects with CLASS_STAR >= {MIN_CLASS_STAR}: {cleaned_count}\")\n",
                "print(f\"Objects removed (CLASS_STAR < {MIN_CLASS_STAR}): {removed_count}\")\n",
                "\n",
                "# ---------------------------------------------------------\n",
                "# Save as a new FITS_LDAC file\n",
                "# ---------------------------------------------------------\n",
                "# We need to preserve the HDU structure for PSFEx to read it\n",
                "hdul = fits.open(catalog_file)\n",
                "hdul[2].data = clean_table.as_array() # Replace data with filtered version\n",
                "hdul.writeto(clean_file, overwrite=True)\n",
                "hdul.close()\n",
                "\n",
                "print(\"-\" * 50)\n",
                "print(f\"✅ CLEANED CATALOG SAVED: {clean_file}\")\n",
                "print(f\"   Contains {len(clean_table)} guaranteed stars.\")\n",
                "print(f\"   Action: Run 'psfex {clean_file}' to build your model.\")\n",
                "print(\"-\" * 50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now use PSFEx to generate a model of the telescope blur ( Point Spread Function ), we hope to find. We attempt FLAG = 0."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Changed snr > 30 over 20, Max_ellp = .2 over .3, and set FLAG to only include perfect stars\n",
                "# ran psfex ForPSF.cat\n",
                "# Analyzed the various statistic fits files"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now attempt to generate parameters through galfit"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Original Shape: (25, 150)\n",
                        "Saved 'final_psf.fits'. Use THIS in GALFIT.\n"
                    ]
                }
            ],
            "source": [
                "from astropy.io import fits\n",
                "\n",
                "# Open the prototype file\n",
                "# Note: Usually the data is in extension 0 or 1.\n",
                "# PSFEx saves the basis images in a 3D cube or a 2D strip.\n",
                "hdul = fits.open('proto_clean_stars.fits') # Or whatever your proto file is named\n",
                "data = hdul[0].data\n",
                "\n",
                "# Check dimensions\n",
                "print(f\"Original Shape: {data.shape}\")\n",
                "\n",
                "# If it's a strip (e.g., 25 pixels tall, 525 pixels wide)\n",
                "# We usually want the FIRST 25x25 box.\n",
                "# Adjust '25' to match your PSF_SIZE from default.psfex\n",
                "psf_size = 25 \n",
                "psf_kernel = data[0:psf_size, 0:psf_size]\n",
                "\n",
                "# Save it as the final PSF for GALFIT\n",
                "fits.PrimaryHDU(data=psf_kernel).writeto('final_psf.fits', overwrite=True)\n",
                "print(\"Saved 'final_psf.fits'. Use THIS in GALFIT.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "frb_project",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
